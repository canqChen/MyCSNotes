## MySQL

### MySQL体系架构

* [详见1](https://blog.csdn.net/u022812849/article/details/107614372)
* [详见2](https://www.cnblogs.com/yanjieli/p/9780100.html)
* [详见3](http://c.biancheng.net/view/7939.html)

### ACID及事务基本概念

* 事务(TRANSACTION)**是一个操作序列，不可分割的工作单位**，以**START TRANSACTION/BEGIN开始，以ROLLBACK/COMMIT**结束

  * 事务特性(ACID)
    * **原子性**(Atomicity)：**逻辑上是不可分割的操作单元**，事务的所有操作**要么全部提交成功，要么全部失败回滚**(用**回滚(Undo Log)日志**实现，反向执行日志中的操作)
    * **一致性**(Consistency)：**数据库在事务执行前后都保持一致性状态**。在一致性状态下，**所有事务对一个数据的读取结果都是相同的**
    * **隔离性**(isolation)：一个事务所做的修改，在最终提交以前，对其他事务都是不可见的
    * **持久性**(Durability)：**一旦事务提交，则所做的修改将永远保存到数据库中**。即使系统发生崩溃，事务执行结果也不会丢失。系统发生崩溃可以用重做日志（Redo Log）进行恢复，从而实现持久性。与**回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改**
  * **只有满足一致性，事务的执行结果才是正确的**。在无并发的情况下，事务串行执行，隔离性一定能够满足。此时**只要能满足原子性，就一定能满足一致性**。在并发的情况下，多个事务并行执行，**事务不仅要满足原子性，还需要满足隔离性，才能满足一致性**。**事务满足持久化是为了能应对系统崩溃的情况**

* **MySQL 默认采用自动提交模式**。也就是说，如果不显式使用`START TRANSACTION`语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交

* **隐式提交**

  * 当我们使用`START TRANSACTION`或者`BEGIN`语句开启一个事务，或者把系统变量`autocommit`的值设为`OFF`时，十五就不会自动提交，但是如果输入了某些语句之后就会悄悄提交，就如输入了`COMMIT`语句一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交
  * 定义或修改数据库对象的数据定义语言（DDL）。数据库对象指的是库、表、视图、存储过程等。使用`CREATE,ALTER,DROP`等语句去修改这些数据库对象时，就会隐式地提交前边语句所属于的事务
  * 隐式使用或修改MySql数据库中的表：使用`ALTER USER,CREATE USER, DROP USER,GRANT,RENAME USER,SET PASSWORD`等语句时也会隐式地提交前面语句所属的事务
  * 事务控制或关于锁定的语句：当一个事务还没提交或者回滚时，又使用`START TRANSACTION`或者`BEGIN`开启另一个事务，会隐式地提交上一个事务。当`AUTOCOMMIT`系统变量的值为`ON`时，也会隐式提交前边语句所属事务。或者使用`LOCK TABLES,UNLOCK TABLES`等关于锁定的语句也会隐式提交前面语句所属的事务
  * 加载数据的语句：使用`LOAD DATA`语句来批量往数据中导入数据时，也会隐式地提交前边语句所属的事务
  * 其他：使用`ANALYZE TABLE, CACHE INDEX, CHECK TABLE, FLUSH, LOAD INDEX INTO CACHE, OPTIMIZE TABLE,REPAIR TABLE, RESET`等语句也会隐式提交前面语句所属事务

* 保存点

  * 当开启了一个事务，并且已经执行了很多语句，忽然发现上一条语句有点问题，这时只好使用`ROLLBACK`语句来让数据库状态回到事务执行之前的样子，然后一切重来，非常不方便

  * MySQL提出一个保存点(savepoint)的概念，就是在事务对应的数据库语句中打几个点，在调用`ROLLBACK`的时候可以指定回到某一个点，而不是回到原点

  * 定义保存点

    ```sqlite
    SAVEPOINT name;
    ```

  * 回滚到某个保存点

    ```sql
    ROLLBACK [WORK] TO [SAVEPOINT] name;
    ```

  * 如果`ROLLBACK`后面不跟随保存点名称，也会直接回滚到事务执行之前的状态

  * 如果像删除某个保存点，可以使用以下语句

    ```sql
    RELEASE SAVEPOINT name;
    ```

* **并发一致性问题**：在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题

  * **丢失修改**：**一个事务对数据进行了修改，在事务提交之前，另一个事务对同一个数据进行了修改，覆盖了之前的修改**。(例如：T1 和 T2 两个事务都对一个数据进行修改，T1 先修改并提交生效，T2 随后修改，T2 的修改覆盖了 T1 的修改。)
  * **脏读**（Dirty Read）：**一个事务读取了被另一个事务修改、但未提交（进行了回滚）的数据，造成两个事务得到的数据不一致**。(例如：T1 修改一个数据但未提交，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。)
  * **不可重复读**（Nonrepeatable Read）：**在同一个事务中，某查询操作在一个时间读取某一行数据和之后一个时间读取该行数据，发现数据已经发生修改**（针对**update**操作）。(例如：T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。)
  * **幻读**（Phantom Read）：**当同一查询多次执行时，由于其它事务在这个数据范围内执行了插入或删除操作，会导致每次返回不同的结果集**（和不可重复读的区别：针对的是一个数据整体/范围；并且针对**insert/delete**操作）(T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同)

* 数据库**四种隔离级别**

  * **未提交读**（Read Uncommited）：在一个事务提交之前，它的执行结果对其它事务也是可见的。**会导致脏读、不可重复读、幻读**

  * **提交读**（Read Commited）：一个事务只能看见**已经提交的事务**所作的改变。**可避免脏读问题**

  * **可重复读**（Repeatable Read）：可以确保同一个事务在**多次读取同样的数据时得到相同的结果**。（MySQL的默认隔离级别）。可避免不可重复读

  * **可串行化**（Serializable）：强制事务串行执行，使之不可能相互冲突，从而解决幻读问题。可能导致大量的超时现象和锁竞争，实际很少使用

  * 设置方法：

    ```sql
    SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE];
    ```

### 锁分类及概念

* **什么是乐观锁和悲观锁？**

  - 悲观锁：**认为数据随时会被修改，因此每次读取数据之前都会上锁，防止其它事务读取或修改数据**；应用于**数据更新比较频繁**的场景；
  - 乐观锁：操作数据时不会上锁，但是**更新时会判断在此期间有没有别的事务更新这个数据，若被更新过，则失败重试；适用于读多写少的场景**。乐观锁的**实现方式**有：
    - **加一个版本号或者时间戳字段，每次数据更新时同时更新这个字段**
    - **先读取想要更新的字段或者所有字段，更新的时候比较一下，只有字段没有变化才进行更新**

* MySQL锁分类？**MySQL大致可归纳为以下3种锁**：

  * **表级锁**：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低
  * **行级锁**：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高
  * **页面锁**：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般(使用页级锁定的主要BerkeleyDB存储引擎)
  * **锁的作用：用于管理对共享资源的并发访问，保证数据库的完整性和一致性**

* 封锁粒度

  * MySQL 中主要提供了两种封锁粒度：行级锁以及表级锁
  * 应该尽量**只锁定需要修改的那部分数据**，而不是所有的资源。**锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高**
  * 但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。**因此封锁粒度越小，系统开销就越大**
  * 在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡

* 常见封锁类型

  * **排他锁**（Exclusive Lock）/ X锁：事务对数据加上X锁时，**只允许此事务读取和修改此数据**，并且**其它事务不能对该数据加任何锁**

  * **共享锁**（Shared Lock）/ S锁：加了S锁后，**该事务只能对数据进行读取而不能修改**，并且**其它事务只能加S锁，不能加X锁**

  * **意向锁**（Intention Lock）：意向锁在原来的 X/S 锁之上引入了 IX/IS。**IX/IS 都是表锁**，*用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁*

    - 一个事务在获得某个**数据行**对象的 S 锁之前，必须**先获得整个表的 IS 锁或更强的锁**

    - 一个事务在获得某个**数据行**对象的 X 锁之前，必须**先获得整个表的 IX 锁**

    - 锁的兼容性：

      |                | 共享锁(S) | 排他锁(X) | 意向共享锁(IS) | 意向排他锁(IX) |
      | :------------: | :-------: | :-------: | :------------: | :------------: |
      |   共享锁(S)    |   兼容    |   冲突    |      兼容      |      冲突      |
      |   排他锁(X)    |   冲突    |   冲突    |      冲突      |      冲突      |
      | 意向共享锁(IS) |   兼容    |   冲突    |      兼容      |      兼容      |
      | 意向排他锁(IX) |   冲突    |   冲突    |      兼容      |      兼容      |

    - **任意 IS/IX 锁之间都是兼容的**，因为它们只表示**想要对表加锁**，而不是真正加锁。这里兼容关系针对的是表级锁，**而表级的 IX 锁和行级的 X 锁兼容**，两个事务可以对两个数据行加 X 锁(行X锁)。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改）

    - 好处：如果一个事务想要对整个表加X锁，就需要先检测是否有其它事务对该表或者该表中的任意一行加了锁(逐行检测)，这种检测非常耗时。有了意向锁之后，**只需要检测整个表是否存在IX/IS/X/S锁**就行了，如果加了就表示**有其它事务正在使用这个表或者表中某一行的锁**

  * **当一个事务需要给自己需要的某个资源加锁的时候，如果遇到一个共享锁正锁定着自己需要的资源的时候，自己可以再加一个共享锁，不过不能加排他锁；但是，如果遇到自己需要锁定的资源已经被一个排他锁占有之后，则只能等待该锁定释放资源之后自己才能获取锁定资源并添加自己的锁定**

  * **而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在**

* 如何**加锁**？

  * MyISAM在执行**查询**语句（SELECT）前，会**自动**给涉及的**所有表加共享锁**(S锁)，在执行**更新操作**（UPDATE、DELETE、INSERT等）前，会**自动**给涉及的表加**排他锁**(X锁)，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁(**MyISAM只支持表锁**)

  * **意向锁是 InnoDB 自动加的**， 不需用户干预。 **对于 UPDATE、 DELETE 和 INSERT 语句**， InnoDB 会**自动给涉及数据集加排他锁**（X)； 对于**普通 SELECT 语句**，InnoDB **不会加任何锁**

  * 事务可以通过以下语句显式给记录集加共享锁或排他锁

    ```sql
    共享锁(S)：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE
    排他锁(X)：SELECT * FROM table_name WHERE ... FOR UPDATE
    ```

    * 用`SELECT ... LOCK IN SHARE MODE`获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行`UPDATE`或者`DELETE`操作。 其他 session 仍然可以查询记录，并也可以对该记录加 共享锁。**但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁**。对于锁定行记录后需要进行更新操作的应用，应该使用`SELECT... FOR UPDATE`方式获得排他锁
    * `SELECT * FROM table_name WHERE ... FOR UPDATE`获得排他锁，其他session可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁
    * `for update` 可以根据条件来完成行锁锁定，**并且锁的对象是有索引键的列(因为行锁是加在索引上)**，**如果锁的对象不是索引键那么InnoDB将完成表锁，并发将无从谈起**

* 什么时候使用表锁？

  * **对于InnoDB表，在绝大部分情况下都应该使用行级锁**，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个别特殊事务中，也可以考虑使用表级锁

    - 第一种情况是：事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度
    - 第二种情况是：事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销
    - 当然，应用中这两种事务不能太多，否则，就应该考虑使用ＭyISAＭ表

  * 在InnoDB下 ，使用表锁要注意以下两点

    * 使用`LOCK TALBES`虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层ＭySQL Server负责的，仅当`autocommit=0、innodb_table_lock=1`（默认设置）时，InnoDB层才能知道MySQL加的表锁，ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁；否则，InnoDB将无法自动检测并处理这种死锁

    * 在用`LOCK TABLES`对InnoDB锁时要注意，要将`AUTOCOMMIT`设为0，**否则ＭySQL不会给表加锁**；事务结束前，不要用`UNLOCK TABLES`释放表锁，因为`UNLOCK TABLES`会隐含地提交事务；`COMMIT`或`ROLLBACK`不能释放用`LOCK TABLES`加的表级锁，必须用`UNLOCK TABLES`释放表锁，正确的方式见如下语句。例如，如果需要写表t1并从表t读，可以按如下做：

      ```sql
      SET AUTOCOMMIT=0;
      LOCK TABLES t1 WRITE, t2 READ, ...;
      [do something with tables t1 and here];
      COMMIT;
      UNLOCK TABLES;
      ```

* 死锁

  * MyISAM表锁是**deadlock free**的，这是因为MyISAM总是**一次获得所需的全部锁，要么全部满足，要么等待**，因此不会出现死锁。但在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，当两个事务都需要获得对方持有的排他锁才能继续完成事务，这种循环锁等待就是典型的死锁
  * 在InnoDB的事务管理和锁定机制中，有专门检测死锁的机制，**会在系统中产生死锁之后的很短时间内就检测到该死锁的存在**。当InnoDB检测到系统中产生了死锁之后，InnoDB会通过**相应的判断来选这产生死锁的两个事务中较小的事务来回滚，而让另外一个较大的事务成功完成**

* 封锁协议

  * 一级封锁协议

    * 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁0

    * 可以解决**丢失修改**问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/first_level_lock.png" alt="first_level_lock" style="zoom:50%;" />

  * 二级封锁协议

    * **在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁**

    * 可以解决**读脏数据**问题，因为如果一个事务在对数据 A 进行修改，根据 一 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/second_level_lock.png" alt="second_level_lock" style="zoom:50%;" />

  * 三级封锁协议

    * 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁

    * 可以解决**不可重复读**的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/third_level_lock.png" alt="third_level_lock" style="zoom:50%;" />

* 两段锁协议

  * 事务必须严格分为两个阶段对数据进行**加锁和解锁**的操作，第一阶段加锁，第二阶段解锁。也就是说一个事务中一旦释放了锁，就不能再申请新锁了

  * **可串行化调度**是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。事务遵循两段锁协议是保证可串行化调度的充分条件

  * 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度

    ```sql
    lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B)
    ```

  * 但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度

    ```sql
    lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C)
    ```

### [MVCC](https://baijiahao.baidu.com/s?id=1629409989970483292&wfr=spider&for=pc)

* [MVCC(多版本并发控制，Multi-Version Concurrency Control)](https://www.jianshu.com/p/8845ddca3b23)

  * MVCC在每行记录后面都保存有两个隐藏的列，用来存储**创建版本号**和**删除版本号**（都是事务版本号）

  * 创建版本号：创建一个数据行时的事务版本号（**事务版本号**：事务开始时的系统版本号；**系统版本号**：每开始一个新的事务，系统版本号就会自动递增）

  * 删除版本号：**删除操作时的事务版本号**

  * 各种操作：

    * 在 MVCC 中事务的**修改操作**（DELETE、INSERT、UPDATE）会为**数据行新增一个版本快照(事务版本号)**
    * 插入操作时，记录创建版本号
    * 删除操作时，记录删除版本号
    * 更新操作时，先记录删除版本号，再新增一行记录创建版本号
    * **查询操作时，要符合以下条件才能被查询出来：删除版本号未定义或大于当前事务版本号**（删除操作是在当前事务启动之后做的）；**创建版本号小于或等于当前事务版本号**（创建操作是事务完成或者在事务启动之前完成）

  * 通过版本号减少了锁的争用，**提高了系统性能**；可以实现**提交读**和**可重复读**两种隔离级别，未提交读无需使用MVCC。脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务**进行读取操作**时，为了解决**脏读和不可重复读**问题，MVCC 规定**只能读取已经提交的快照**。当然一个事务可以读取自身未提交的快照，这不算是脏读

  * 而 MVCC 利用了多版本的思想，**写操作更新最新的版本快照，而读操作去读旧版本快照**，没有互斥关系，这一点和 Copy On Write 类似

  * **快照读与当前读**

    * **使用 MVCC 读取的是快照中的数据**，这样可以减少加锁所带来的开销

      ```sql
      select * from table ...;
      ```

    * 当前读**读取的是最新的数据，需要加锁**。以下第一个语句需要加 S 锁，其它都需要加 X 锁。**MVCC 对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作**

      ```sql
      select * from table where ? lock in share mode; -- S锁
      select * from table where ? for update;		--X锁
      insert;
      update;
      delete;
      ```

* **ReadView **

  * MVCC 维护了一个 ReadView 结构，主要包含了**当前系统未提交的事务列表** (`list`)TRX_IDs {TRX_ID_1, TRX_ID_2, ...}，还有该列表的最小值 (`up_limit_id`)TRX_ID_MIN 和**最小尚未分配的事务id号**(`lower_limit_id`)TRX_ID_MAX

    ![readview](https://gitee.com/canqchen/cloudimg/raw/master/img/readview.png)

  * 在进行 SELECT 操作时，**根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用**：

    - TRX_ID < TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此**可以使用**
    - TRX_ID > TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此**不可使用**
    - TRX_ID_MIN <= TRX_ID <= TRX_ID_MAX，需要根据隔离级别再进行判断：
      - 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务**还未提交**，则该快照不可使用。否则表示已经提交，可以使用
      - 可重复读：**都不可以使用**。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题

    **在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断**

  * mvcc通过控制**readview的生成时机**实现RC和RR隔离级别
    * 对于RC隔离级别，**每次执行快照读都生成一个readview**，可以读到最新提交的快照，实现RC，同时导致不可能重复读
    * 对于RR隔离级别，**只在第一次快照读生成一个readview**，后续的快照读都使用第一次生成的readview，从而实现可重复读

* **在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Lock 防止幻读**

* Next-Key Lock

  * Next-Key Lock 是 MySQL 的 InnoDB 存储引擎的一种锁实现。**MVCC 不能解决幻影读问题**，Next-Key Lock 就是为了解决这个问题而存在的。**在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Lock 可以解决幻读问题**

* Record Lock

  * **锁定一个记录上的索引，而不是记录本身**。**如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Lock依然可以使用**

* Gap Lock

  * 锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15

    ```sql
    SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;
    ```

* Next-Key Locks

  * 它是 Record Locks 和 Gap Locks 的结合，**不仅锁定一个记录上的索引，也锁定索引之间的间隙**。它锁定一个**前开后闭区间**，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：

    ```sql
    (-∞, 10]
    (10, 11]
    (11, 13]
    (13, 20]
    (20, +∞)
    ```

### 其他概念

* 表的几种连接方式

  * 内连接（Inner Join）：仅将两个表中满足连接条件的行组合起来作为结果集

    - 自然连接：**只考虑属性相同的元组对**
    - 等值连接：给定条件进行查询

  * 外连接（Outer Join）

    - 左连接：左边表的所有数据都有显示出来，右边的表数据只显示共同有的那部分，没有对应的部分补NULL
    - 右连接：和左连接相反
    - 全外连接（Full Outer Join）：**查询出左表和右表所有数据，但是去除两表的重复数据**

  * 交叉连接（Cross Join）：返回两表的笛卡尔积（对于所含数据分别为m、n的表，返回m*n的结果）

    ![sql_join](https://gitee.com/canqchen/cloudimg/raw/master/img/sql_join.png)

* 什么是存储过程(procedure)？有哪些优缺点？

  * 存储过程是**事先经过编译并存储在数据库中的一段SQL语句的集合**。想要实现相应的功能时，只需要调用这个存储过程就行了（类似于函数，输入具有输出参数）

  * 优点：

    - **预先编译，而不需要每次运行时编译**，提高了数据库执行**效率**
    - **封装了一系列操作，对于一些数据交互比较多的操作**，相比于单独执行SQL语句，可以**减少网络通信量**
    - 具有**可复用性**，减少了数据库开发的工作量
    - **安全性高**，可以让没有权限的用户通过存储过程间接操作数据库
    - 更**易于维护**

  * 缺点：

    - **可移植性差**，存储过程将应用程序绑定到了数据库上
    - **开发调试复杂**：没有好的IDE
    - **修改复杂**，需要**重新编译**，有时还需要更新程序中的代码以更新调用

    ```sql
    delimiter //
    
    create procedure myprocedure( out ret int )
        begin
            declare y int;
            select sum(col1)
            from mytable
            into y;
            select y*y into ret;
        end //
    
    delimiter ;
    
    call myprocedure(@ret);
    select @ret;
    ```

* Drop/Delete/Truncate的区别？

  - **Delete**用来删除表的全部或者**部分数据**，执行delete之后，用户**需要提交**之后才会执行，会触发表上的DELETE**触发器**（包含一个OLD的虚拟表，可以只读访问被删除的数据），**DELETE之后表结构还在，删除很慢，一行一行地删，因为会记录日志，可以利用日志还原数据**

  - **Truncate删除表中的所有数据**，这个操作**不能回滚**，也不会触发这个表上的触发器。操作比DELETE快很多（直接把表drop掉，再创建一个新表，删除的数据不能找回）。如果表中有自增（AUTO_INCREMENT）列，则重置为1

  - **Drop**命令从数据库中**删除表**，所有的数据行，索引和约束都会被删除；不能回滚，不会触发触发器

    ![ddt_comp](https://gitee.com/canqchen/cloudimg/raw/master/img/ddt_comp.png)

* **in和exists的区别**

  * mysql中的**in语句是把外表和内表作hash 连接**，而**exists语句是对外表作loop循环**，**每次loop循环再对内表进行查询**。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的
    * 如果查询的**两个表大小相当**，那么用**in和exists差别不大**
    * 如果**两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in**
  * not in 和not exists：如果查询语句使用了not in，那么**内外表都进行全表扫描，没有用到索引**；**而not exists的子查询依然能用到表上的索引**。所以无论那个表大，用not exists都比not in要快

* 什么是触发器？

  * 触发器（TRIGGER）是由事件（比如INSERT/UPDATE/DELETE）来触发运行的操作（不能被直接调用，不能接收参数）。在数据库里以独立的对象存储，用于**保证数据完整性**（比如可以检验或转换数据）

* 有哪些约束类型？

  * NOT NULL: 用于控制字段的内容一定不能为空（NULL）
  * UNIQUE: **控件字段内容不能重复**，一个表允许有多个 Unique 约束
  * PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个
  * FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。**通过定义外键约束，关系数据库可以保证无法插入无效的数据**
  * CHECK: 用于控制字段的值范围

* 什么是视图？什么是游标？

  * 视图：**从数据库的基本表中通过查询选取出来的数据组成的虚拟表**（数据库中存放视图的定义）。**可以对其进行增/删/改/查等操作**。**视图是对若干张基本表的引用，一张虚表，查询语句执行的结果，不存储具体的数据（基本表数据发生了改变，视图也会跟着改变）**；可以跟基本表一样，进行增删改查操作(ps:增删改操作有条件限制)；如连表查询产生的视图无法进行，对视图的增删改会影响原表的数据。好处：

    - 通过**只给用户访问视图的权限**，保证数据的**安全性**
    - **简化**复杂的SQL操作，**隐藏数据的复杂性**（比如复杂的连接）

    ```sql
    CREATE VIEW myview AS
    SELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_col
    FROM mytable
    WHERE col5 = val;
    ```

  * 游标（Cursor）：用于定位在查询返回的**结果集的特定行**，以对特定行进行操作。**使用游标可以方便地对结果集进行移动遍历，根据需要滚动或对浏览/修改任意行中的数据。主要用于交互式应用**

### 主从复制

* 什么是主从复制？实现原理是什么？

  * 主从复制（Replication）是指数据可以**从一个MySQL数据库主服务器复制到一个或多个从服务器**，从服务器可以复制主服务器中的所有数据库或者特定的数据库，或者特定的表。**默认采用异步模式**

  * 实现原理：

    - 主服务器 **binary log dump 线程**：**将主服务器中的数据更改（增删改）日志写入 Binary log** 中

    - 从服务器 **I/O 线程**：**负责从主服务器读取binary log，并写入本地的 Relay log**

    - 从服务器 **SQL 线程**：负责**读取 Relay log，解析出主服务器已经执行的数据更改，并在从服务器中重新执行（Replay），保证主从数据的一致性**

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/master-replica.png" alt="master-replica" style="zoom:80%;" />

* 为什么要主从复制？

  * **读写分离**：主服务器负责写，从服务器负责读
    - **缓解了锁的争用**，**即使主服务器中加了锁，依然可以进行读操作**
    
    - **从服务器可以使用 MyISAM，提升查询性能以及节约系统开销**
    
    - **增加冗余，提高可用性**
    
    - **读写分离常用代理方式来实现**，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器
    
      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/rw_split.png" alt="rw_split" style="zoom:80%;" />
  * **数据实时备份，当系统中某个节点发生故障时，可以方便的故障切换**
  * **降低单个服务器磁盘I/O访问的频率，提高单个机器的I/O性能**

* **主从复制的不同模式及区别**

  ![master-slave](https://gitee.com/canqchen/cloudimg/raw/master/img/master-slave.jpg)

* 主从复制延迟产生的原因
  * 在某些部署环境中，备库所在的机器性能要比主库所在的机器性能差。此时如果**机器的资源不足的话，就会影响备库同步的效率**
  * 备库充当了读库，一般情况下主要写的压力在主库，那么备库会提供一部分读的压力，而如果**备库的查询压力过大**的话，备库的查询消耗了大量的cpu资源，那么必不可少的就会影响同步的速度
  * **大事务执行**，如果主库的一个事务执行了10分钟，而binlog的写入必须要等待事务完成之后，才会传入备库，那么此时在开始执行的时候就已经延迟了10分钟了
  * 主库的写操作是顺序写binlog，从库单线程去主库顺序读binlog，从库取到binlog之后在本地执行。mysql的主从复制都是单线程操作，但是由于主库是顺序写，所以效率很高，而从库也是顺序读取主库的binlog日志，此时的效率也是比较高的，**但是当数据拉去回来之后变成了随机的读写操作**，而不是顺序的，此时也会发生延迟
  * 从库在同步数据的同时，可能**跟其他查询的线程发生锁抢占**的情况，此时也是会发生延迟
  * 当从库的TPS并发非常高的情况下，产生的DDL数量超过了一个线程所能承受的范围的时候，那么也可能导致延迟
  * 在进行binlog日志传输的时候，如果**网络带宽不是很好**，那么**网络延迟**也可能造成数据同步延迟
  
* 主从复制延迟解决办法

  * 架构方面

    * **业务的持久化层实现采用分库架构，让不同的业务请求分散到不同的数据库服务上，分散单台机器的压力**
    * 服务的基础架构在服务和mysql之间加入缓冲层，减少mysql读的压力，但是需要注意的是，**如果数据经常要发生修改，那么这种设计是不合理的**，因为**需要频繁的去更新缓存中的数据**，保持数据的一致性，**导致缓存的命中率很低**，所以此时就要慎用缓存
    * 使用更好的硬件设备，比如cpu，ssd等，但是这种方案一般对于公司而言不太能接受，原因很简单，就是会增加公司的成本，需要评估投入产出比

  * 并行复制

    ![parrarel_copy](https://gitee.com/canqchen/cloudimg/raw/master/img/parrarel_copy.png)

    * MySQ 5.6版本后引入了并行复制的概念
    * 所谓的并行复制，就是在中间添加了一个分发环节，也就是原来的sql_thread变成了coordinator组件，当日志来了之后，coordinator负责读取日志信息和分发事务，真正的日志执行的过程是放在了worker线程上，由**多个线程并行执行**
    * 分发规则
      * 更新同一行的多个事务，必须要分发到同一个worker中执行，否则会引起数据不一致的问题
      * 同一个事务不能被拆开，必须分发给同一个worker执行

* 数据更新流程

  ![data_update](https://gitee.com/canqchen/cloudimg/raw/master/img/data_update.png)

  * 执行流程
    * 执行器先从引擎中找到数据，如果在内存中直接返回，不在，查询后返回
    * 执行器拿到数据之后会先修改数据，然后调用引擎接口重新写入数据
    * 引擎将数据更新到内存，同时写数据到redolog中，此时**处于prepare阶段**，并通知执行器执行完成，随时可以操作
    * 执行器生成这个操作的binlog
    * 执行器调用引擎的事务提交接口，引擎把刚写完的redolog改成**commit状态**，更新完成
  * **redo log两阶段提交**
    * 先写redo log后写binlog：假设在redo log写完，binlog还没有写完的时候，mysql进程异常重启。由于在redo log写完后，系统即使崩溃，仍然能后把数据恢复回来，但由于binlog没写完就crash了，这时候binlog就没有记录完修改语句。因此之后备份日志的时候，存起来的binlog里面语句不全。如果使用这个binlog恢复临时库，会与主库的数据不一致
    * 先写binlog后写redo log：如果在binlog写完后crash，由于redo log还没写，崩溃恢复以后这个事务无效，但是binlog已经记录了完整的事务日志。所以之后用binlog恢复从库数据时，与主库数据也不一致

### 索引

* InnoDB的索引使用的是B+树实现，**B+树对比B树的好处**：

  - **IO次数少**：B+树的中间结点只存放索引，数据都存在叶结点中，因此中间结点可以存更多的索引数据，让索引树更加矮胖

  - **范围查询效率更高**：B树需要中序遍历整个树，只B+树需要遍历叶结点中的链表

  - **查询效率更加稳定**：每次查询都需要从根结点到叶结点，路径长度相同，所以每次查询的效率都差不多

    ![b+tree](https://gitee.com/canqchen/cloudimg/raw/master/img/b+tree.jpg)

* 使用**B树索引和哈希索引的比较**

  * **哈希索引能以 O(1) 时间进行查找，但是只支持精确查找，无法用于部分查找和范围查找，无法用于排序与分组**；B树索引支持大于小于等于查找，范围查找。哈希索引遇到大量哈希值相等的情况后查找效率会降低。**哈希索引不支持数据的排序**
  * InnoDB 存储引擎有一个特殊的功能叫“**自适应哈希索引**”，当**某个索引值被使用的非常频繁**时，会在 **B+Tree 索引之上再创建一个哈希索引**，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找

* **B+树与红黑树的比较**

  *红黑树等平衡树也可以用来实现索引*，但是**文件系统及数据库系统普遍采用 B+ Tree 作为索引结构**，这是因为**使用 B+ 树访问磁盘数据有更高的性能**

  * **B+ 树有更低的树高**
    * 平衡树的树高 $\textbf{O}(h)=\textbf{O}(\log_dN)$，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 *h* 很明显比 B+ Tree 大非常多
  * **磁盘访问原理**
    * 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页(一般为4kB)，内存与磁盘**以页为单位交换数据**。**数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点**
    * **Innodb页大小默认为16kB，MySQL一次载入一个InnoDB页**
    * 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。**B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比**，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取
  * **磁盘预读特性**
    * **为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读**。预读过程中，**磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入**

* B+树索引

  * 是大多数 MySQL 存储引擎的默认索引类型

  * 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多

  * 适用于**全键值、键值范围和键前缀查找**，其中**键前缀查找只适用于最左前缀查找**。**如果不是按照索引列的顺序进行查找，则无法使用索引**

  * InnoDB 的 B+Tree 索引分为**主索引和辅助索引**。**主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引**。因为**无法把数据行存放在两个不同的地方**，所以**一个表只能有一个聚簇索引**

    ![main_idx](https://gitee.com/canqchen/cloudimg/raw/master/img/main_idx.png)

  * **辅助索引也叫非聚簇索引**的叶子节点的 **data 域记录着主键的值**，因此在**使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找**

    ![aux_idx](https://gitee.com/canqchen/cloudimg/raw/master/img/aux_idx.png)

* 全文索引

  * MyISAM 存储引擎支持全文索引，用于**查找文本中的关键词，而不是直接比较是否相等**
  * 查找条件使用 `MATCH AGAINST`，而不是普通的 `WHERE`
  * 全文索引使用**倒排索引**实现，它记录着**关键词到其所在文档的映射**
  * InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引

* 覆盖索引

  * 索引**包含所有需要查询的字段的值**。具有以下优点：
    - 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量
    - 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）
    - **对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引(回表)**

* 使用索引的优点和缺点

  * 大大加快了数据的**检索速度**
  * 可以显著减少查询中**分组和排序**的时间
  * 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性
  * 将随机 I/O 变为**顺序 I/O**（B+Tree 索引是有序的，会将相邻的数据都存储在一起）
  * **缺点**：**建立和维护索引耗费时间空间，更新索引很慢**

* [**聚簇索引和非聚簇索引区别?**](https://www.imooc.com/article/details/id/289139)

  * **聚簇索引(clustered index)**
    * 概念
      * 聚簇索引就是按照每张表的主键构造一棵B+树，同时**叶子节点中存放的就是整张表的行记录数据，存储数据的顺序和索引顺序一致**，也将聚集索引的**叶子节点称为数据页**，**在记录插入的时候，会对数据页重新排序**
      * 一个表只能有一个聚簇索引，因为在一个表中数据的存放方式只有一种
    * 优点
      * 由于**行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问**，不必访问磁盘。这样**主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回**了，**如果按照主键Id来组织数据，获得数据更快**
      * 聚簇索引**对于索引列的排序查找和范围查找速度非常快**，因为其数据是按照大小排列的
    * 缺点
      * **聚簇索引的更新代价比较高**，如果更新了行的聚簇索引列，就需要将数据移动到相应的位置。这可能因为要插入的页已满而导致“页分裂”。因此，对于InnoDB表，我们一般定义主键为不可更新
      * 聚簇索引可能导致全表扫描速度变慢，因为可能需要加载物理上相隔较远的页到内存中（需要耗时的磁盘寻道操作）
      * 插入速度严重依赖于插入顺序，**按照主键进行插入的速度是加载数据到Innodb中的最快方式，否则将会出现页分裂，严重影响性能**。因此，对于InnoDB表，我们一般都会定义一个**自增的ID列为主键**。如果不是按照主键插入，最好在加载完成后使用`OPTIMIZE TABLE`命令重新组织一下表
      * **维护索引很昂贵，特别是插入新行或者主键被更新导致页分裂(page split)问题**
      * 表因为使用UUId（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫面更慢，所以建议使用int的auto_increment作为主键
  * **非聚簇索引(nonclustered index)**
    * 概念
      * Innodb非聚簇索引的叶子节点并**不包含行记录的全部数据**，**叶子节点除了包含键值外，还包含了相应行数据的聚簇索引键**
      * 非聚簇索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个非聚簇索引。在innodb中有时也称非聚簇索引为二级索引
    * 优点
      * 非聚簇叶子节点数据部分只存储了聚簇索引键值，而不是行指针(row pointers)，这减小了移动数据或者数据页面分裂时维护非聚簇索引的开销，因为不需要更新索引的行指针
    * 缺点
      * 非聚簇索引的二次查询（回表）问题：**二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据**
    * **建立复合索引（多列索引，组合索引）解决二次查询问题**
  * **根本区别：**
    * 聚簇索引和非聚簇索引的**根本区别**是**索引和表数据是否存放在一起**

* Innodb和Myisam索引实现

  * Innodb索引实现

    * 主键索引

      * 在**InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构**，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引

      * 因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则**MySQL自动为InnoDB表生成一个隐含字段(rowid)作为主键，这个字段长度为6个字节，类型为长整形**

        ![innodb_primary](https://gitee.com/canqchen/cloudimg/raw/master/img/innodb_primary.png)

    * 辅助索引

      * InnoDB 表是基于聚簇索引建立的，因此InnoDB 的索引能提供一种非常快速的主键查找性能。InnoDB的所有非聚簇索引都引用聚簇索引键值作为data域，所以，**如果主键定义的比较大，其他索引也将很大**。如果想在表上定义很多索引，则争取尽量把主键定义得小一些。**InnoDB 不会压缩索引**

        ![innodb_secondry](https://gitee.com/canqchen/cloudimg/raw/master/img/innodb_secondry.png)

  * Myisam索引实现

    * 主键索引

      * MyISAM引擎使用B+Tree作为索引结构，**MyISAM索引文件和数据文件是分离的**，叶节点的**data域存放的是数据记录的地址**

        <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/myisam_primary.png" alt="myisam_primary" style="zoom:67%;" />

    * 辅助索引

      * 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，**只是主索引要求key是唯一的，而辅助索引的key可以重复**

      * 同样也是一颗B+Tree，data域保存数据记录的地址。因此，**MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录**

        <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/myisam_secondry.jpg" alt="myisam_secondry" style="zoom:67%;" />

  * Innodb和Myisam索引差别

    <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/idx_diff.png" alt="idx_diff" style="zoom:80%;" />

* **一个表中无论索引有多少个，数据只存储一份**。**数据在进行插入时，是必须跟索引存储在一起的。在Innodb存储引擎中，如果表中有主键，则用主键建立聚簇索引；如果没有主键，则用唯一键作为主键建立聚簇索引；如果还是没有唯一键，则Innodb会自动创建一个6字节的rowid字段为主键，作为聚簇索引。数据与聚簇索引存放一起**。无论如何，一定会建立一个聚簇索引

* **为什么官方建议使用自增长主键作为索引？**

  * 结合B+Tree的特点，**自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分**。并且能减少数据的移动，每次插入都是插入到最后。总之就是**减少分裂和移动的频率**

* **索引失效**

  * 以“%(表示任意0个或多个字符)”开头的LIKE语句
  * OR语句**前后没有同时使用索引**
  * **数据类型出现隐式转化**（如varchar不加单引号的话可能会自动转换为int型）
  * 对于组合索引，必须满足 **最左匹配原则**/**最左前缀原则** (最左优先，eg：多列索引col1、col2和col3，则索引生效的情形包括 col1或col1，col2或col1，col2，col3)
  * 如果MySQL估计**全表扫描比索引快，则不使用索引**（比如**非常小的表**）

* **最左前缀原则**

  * 创建组合索引：`ALTER TABLE tablename ADD INDEX lname_fname_age (lame, fname, age);`
  * 为了提高搜索效率，我们需要考虑运用多列索引，由于索引文件以B+Tree格式保存，所以我们不用扫描任何记录，即可得到最终结果
  * 注：在mysql中执行查询时，只能使用一个索引，如果我们在lname，fname，age上分别建索引，执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引
  * **最左前缀原则：**顾名思义，就是最左优先，上例中我们创建了lname_fname_age多列索引，**相当于创建了(lname)单列索引，(lname，fname)组合索引以及(lname，fname，age)组合索引**
  * mysql会**一直向右匹配直到遇到范围查询**(**>、<、between、like**)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整

* [**索引下推**](https://blog.csdn.net/sinat_29774479/article/details/103470244)

  * 概念

    * 索引下推（index condition pushdown ）简称ICP，在Mysql5.6的版本上推出，用于优化查询

    * 在不使用ICP的情况下，**在使用非主键索引（又叫普通索引或者二级索引）进行查询时**，**存储引擎通过索引检索到数据，然后返回给MySQL服务器，服务器然后判断数据是否符合条件** 

      ![wo_icp](https://gitee.com/canqchen/cloudimg/raw/master/img/wo_icp.png)

    * **在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器** 

      ![w_icp](https://gitee.com/canqchen/cloudimg/raw/master/img/w_icp.png)

    * **索引条件下推优化可以减少存储引擎查询基础表的次数，也可以减少MySQL服务器从存储引擎接收数据的次数**

    * 索引下推优化技术其实就是**充分利用了索引中的数据**，**尽量在查询出整行数据之前过滤掉无效的数据**

  * 实例解析

    * 在开始之前先先准备一张用户表(user)，其中主要几个字段有：id、name、age、address。**建立联合索引（name，age）**

    * 假设有一个需求，要求匹配姓名第一个为陈的所有用户，sql语句如下

      ```sql
      SELECT * from user where  name like '陈%'
      ```

    * 根据 "最佳左前缀" 的原则，这里使用了**联合索引**（name，age）进行了查询，性能要比全表扫描肯定要高

    * 问题来了，如果有其他的条件呢？假设又有一个需求，要求匹配姓名第一个字为陈，年龄为20岁的用户，此时的sql语句如下

      ```sql
      SELECT * from user where  name like '陈%' and age=20
      ```

    * 这条sql语句应该如何执行呢？下面对Mysql 5.6之前版本和之后版本进行分析

      * 5.6之前的版本是没有索引下推这个优化的，因此执行的过程如下图

        <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/icp.png" alt="icp" style="zoom:67%;" />

      * 会忽略age这个字段，直接通过name进行查询，在(name,age)这课树上查找到了两个结果，id分别为2,1，然后拿着取到的id值一次次的回表查询，因此这个过程需要**回表两次**

      * 5.6版本添加了索引下推这个优化，执行的过程如下图

        <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/icp2.png" alt="icp2" style="zoom:67%;" />

      * InnoDB并**没有忽略age这个字段**，而是**在索引内部就判断了age是否等于20**，对于不等于20的记录直接跳过，因此在(name, age)这棵索引树中只匹配到了一个记录，此时拿着这个id去主键索引树中回表查询全部数据，这个过程**只需要回表一次**

    * 索引下推在**非主键索引**上的优化，可以**有效减少回表的次数，大大提升了查询的效率**

    * 关闭索引下推可以使用如下命令

    ```sql
    set optimizer_switch='index_condition_pushdown=off';
    ```

* **什么地方适合做索引**

  * **某列经常作为最大最小值**
  * 在**作为主键**的列上，强制该列的唯一性和组织表中数据的排列结构
  * **经常被查询的字段**
  * 在**经常用作表连接的字段**上，这些列**主要是一些外键**，可以加快连接的速度
  * **经常出现在ORDER BY/GROUP BY/DISDINCT后面的字段**
  * 在**经常需要排序**的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间
  * 在**经常使用在WHERE子句**中的列上面创建索引，加快条件的判断速度
  * 在经常需要**根据范围进行搜索**的列上创建索引，因为索引已经排序，其指定的范围是连续的

* 创建索引时需要注意什么？

  * 只应建立在**小字段**上，而不要对大**文本或图片**建立索引（**一页存储的数据越多一次IO操作获取的数据越大效率越高**）
  * 建立索引的字段应该**非空**，在MySQL中，**含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂**。应该用0、一个特殊的值或者一个空串代替NULL
  * 选择**数据密度大**（唯一值占总数的百分比很大）的字段作索引
  * 对于那些在**查询中很少使用或者参考的列不应该创建索引**。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求
  * 对于那些只有**很少数据值的列**也不应该增加索引。这是因为，由于这些列的取值很少，**例如人事表的性别列**，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度
  * 对于那些定义为**text, image和bit数据类型**的列不应该增加索引。这是因为，这些列的**数据量要么相当大，要么取值很少**
  * 当**修改性能远远大于检索性能**时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引

### 范式

* 范式（数据库设计范式）是符合某一种级别的关系模式的集合。构造数据库必须遵循一定的规则。在关系数据库中，这种规则就是范式。关系数据库中的关系必须满足一定的要求，即满足不同的范式
* 目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd范式（BCNF）、第四范式（4NF）和第五范式（5NF）
* 满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多要求的称为第二范式（2NF），其余范式以次类推。**一般说来，数据库只需满足第三范式（3NF）就行了**

#### 三大范式

* 第一范式

  * 在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，**不满足第一范式（1NF）的数据库就不是关系数据库**
  * 所谓第一范式（1NF）是指**数据库表的每一列都是不可分割的基本数据项**，**同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性**
  * 如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息
  * 简而言之，第一范式就是无重复的列
  * 例如，下面的数据库表是符合第一范式的：
    * 字段1 字段2 字段3 字段4
  * 而这样的数据库表是不符合第一范式的：
    * 字段1 字段2 字段3 字段4 字段3.1 字段3.2

* 第二范式

  * 第二范式（2NF）要求**数据库表中的每个实例或行必须可以被唯一地区分**。为实现区分通常需要**为表加上一个列**，以**存储各个实例的唯一标识**。这个唯一属性列被称为**主关键字或主键、主码**

  * 第二范式（2NF）**要求实体的属性完全依赖于主关键字**。所谓**完全依赖是指不能存在仅依赖主关键字一部分的属性**。如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识

  * 简而言之，第二范式就是**非主属性完全依赖于主关键字**

  * 假定选课关系表为`SelectCourse`(学号，姓名，年龄，课程名称，成绩，学分)，**关键字为组合关键字**(学号，课程名称)，因为存在如下决定关系：

    * (学号，课程名称) → (姓名，年龄，成绩，学分)

    * 这个数据库表不满足第二范式，因为存在如下决定关系：

      * (课程名称) → (学分)

      * (学号) → (姓名，年龄)

    * **即存在组合关键字中的字段决定非关键字的情况**

  * 由于不符合2NF，这个选课关系表会存在如下问题：

    * (1) **数据冗余**：
      * 同一门课程由n个学生选修，"学分"就重复n-1次；同一个学生选修了m门课程，姓名和年龄就重复了m-1次

    * (2) **更新异常**：
      * 若调整了某门课程的学分，数据表中所有行的"学分"值都要更新，否则会出现同一门课程学分不同的情况

    * (3) **插入异常**：
      * 假设要开设一门新的课程，暂时还没有人选修。这样，由于还没有"学号"关键字，课程名称和学分也无法记录入数据库

    * (4) **删除异常**：
      * 假设一批学生已经完成课程的选修，这些选修记录就应该从数据库表中删除。但是，与此同时，课程名称和学分信息也被删除了。很显然，这也会导致插入异常

  * 把选课关系表SelectCourse改为如下三个表：

    * 学生：Student(学号，姓名，年龄)

    * 课程：Course(课程名称，学分)

    * 选课关系：SelectCourse(学号，课程名称，成绩)

  * 这样的数据库表是符合第二范式的， 消除了数据冗余、更新异常、插入异常和删除异常
  * 另外，**所有单关键字的数据库表都符合第二范式，因为不可能存在组合关键字**

* 第三范式

  * 满足第三范式（3NF） 必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息
  * 例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在图3-2的员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余
  * 简而言之，第三范式就是**属性不依赖于其它非主属性**
  * 因此，满足第三范式的数据库表应该**不存在如下依赖关系**：关键字段 → 非关键字段x → 非关键字段y
  * 假定学生关系表为Student(学号，姓名，年龄，所在学院，学院地点，学院电话)，关键字为单一关键字"学号"，因为存在如下决定关系：
    * (学号) → (姓名，年龄，所在学院，学院地点，学院电话)
  * 这个数据库是符合2NF的，但是不符合3NF，因为存在如下决定关系：
    * (学号) → (所在学院) → (学院地点，学院电话)
  * 即存在非关键字段"学院地点"、"学院电话"对关键字段"学号"的**传递函数依赖**

  * 它也会存在数据冗余、更新异常、插入异常和删除异常的情况，读者可自行分析得知。

  * 把学生关系表分为如下两个表：

    * 学生：(学号，姓名,年龄，所在学院)

    * 学院：(学院，地点，电话)

  * 这样的数据库表是符合第三范式的，消除了数据冗余、更新异常、插入异常和删除异常

### 存储引擎比较

* MySQL的两种存储引擎 InnoDB 和 MyISAM 的区别？

  * InnoDB**支持事务**，可以进行Commit和Rollback

  * **MyISAM 只支持表级锁**，而 InnoDB 还**支持行级锁**，提高了并发操作的性能

  * InnoDB **支持外键**，Myisam不支持

  * MyISAM **崩溃**后发生损坏的概率比 InnoDB 高很多，而且**恢复的速度**也更慢

  * MyISAM 支持**压缩**表和空间数据索引，InnoDB需要更多的内存和存储

  * MyISAM存储引擎下**索引和数据存储是分离的**，**InnoDB索引和数据存储在一起(聚簇索引)**

  * InnoDB 支持在线**热备份**

  * MyISAM是deadlock-free的，而InnoDB可能会出现死锁现象

  * 应用场景

    * **MyISAM** 管理非事务表。它提供高速存储和检索（MyISAM强调的是性能，**每次查询具有原子性**，其执行速度比InnoDB更快），以及全文搜索能力。如果**表比较小**，或者是**只读数据**（有大量的SELECT），还是可以使用MyISAM
    * **InnoDB** 支持事务，**并发情况下有很好的性能**，基本可以替代MyISAM

  * 热备份和冷备份

    * 热备份：**在数据库运行的情况下备份的方法**。优点：可按表或用户备份，备份时数据库仍可使用，可恢复至任一时间点。但是不能出错

    * 冷备份：**数据库正常关闭后，将关键性文件复制到另一位置的备份方式**。优点：操作简单快速，恢复简单

      ![engine_comp](https://gitee.com/canqchen/cloudimg/raw/master/img/engine_comp.jpg)

### MySQL优化

* 优化数据库的方法

  * SQL **语句的优化**

    * **分析慢查询日志**：记录了在MySQL中响应时间超过阀值`long_query_time`的SQL语句，通过日志去找出IO大的SQL以及发现未命中索引的SQL

      * 首先分析语句，看看**是否load了额外的数据**，可能是查询了**多余的行并且抛弃掉了**，可能是加载了**许多结果中并不需要的列**，对语句进行分析以及重写
      * 分析语句的执行计划，然后**获得其使用索引的情况**，之后**修改语句或者修改索引**，**使得语句可以尽可能的命中索引**
      * 如果对语句的优化已经无法进行，可以**考虑表中的数据量是否太大**，如果是的话可以进行横向或者纵向的分表

    * **只返回必要的列**：最好不要使用 `SELECT *` 语句

    * **UNION ALL的效率高于UNION**

    * 只**返回必要的行**：使用 **LIMIT 语句**来限制返回的数据，可以记录上次查询的最大ID，下次查询直接根据ID来查询

    * 将一个**大连接查询分解成对每一个表进行一次单表查询**，然后在应用程序中进行关联，这样做的好处有

      - **让缓存更高效**。对于连接查询，**如果其中一个表发生变化，那么整个查询缓存就无法使用**。**而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用**
      - 分解成**多个单表查询**，这些单表查询的缓存结果更可能被其它查询使用到，**从而减少冗余的查询**
      - **减少锁竞争**

    * **优化WHERE子句**

      * 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引

      * 应尽量**避免在 where 子句中对字段进行 null 值判断**，否则将导致引擎放弃使用索引而进行全表扫描

      * 应尽量**避免在 where 子句中使用!=或<>操作符**，否则引擎将放弃使用索引而进行全表扫描

      * 应尽量避免在 where 子句中**使用or 来连接条件**，否则将导致引擎放弃使用索引而进行全表扫描

        ```sql
        select id from t where num=10 or num=20;
        -- 可以这样查询：
        select id from t where num=10 
        union all 
        select id from t where num=20;
        ```

      * in 和 not in 也要慎用，否则会导致全表扫描

      * 下面的查询也将导致全表扫描：`select id from t where name like '%李%'`若要提高效率，可以考虑全文检索

      * **如果在 where 子句中使用参数，也会导致全表扫描**。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描

        ```sql
        select id from t where num=@num;
        -- 可以改为强制查询使用索引：
        select id from t with(index(索引名)) where num=@num;
        ```

      * 应尽量避免在 where 子句中**对字段进行表达式操作**，这将导致引擎放弃使用索引而进行全表扫描

        ```sql
        select id from t where num/2=100
        -- 应改为:
        select id from t where num=100*2
        ```

      * 应尽量避免在where子句中**对字段进行函数操作**，这将导致引擎放弃使用索引而进行全表扫描

        ```sql
        select id from t where substring(name,1,3)=’abc’;
        -- name以abc开头的id应改为:
        select id from t where name like ‘abc%’;
        ```

      * 不要在 where 子句中的**“=”左边进行函数、算术运算或其他表达式运算**，否则系统将可能无法正确使用索引

    * **优化长难的查询语句**

      * MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多
      * 使用**尽可能小的查询是好的**，但是有时将一个**大的查询分解为多个小的查询**是很有必要的
      * **切分查询，将一个大的查询分为多个小的相同的查询**
      * **一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销**
      * 分解**关联查询，让缓存的效率更高**
      * **执行单个查询可以减少锁的竞争**
      * 在应用层做关联更容易对数据库进行拆分，查询效率会有大幅提升
      * 较少冗余记录的查询

    * 优化**特定类型**的查询语句

      * **count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名)**
      * MyISAM中，没有任何where条件的`count(*)`非常快
      * 当有where条件时，MyISAM的count统计不一定比其它引擎快
      * 可以使用explain查询近似值，用近似值替代count(*)
      * 增加汇总表
      * **使用缓存**

    * 优化**关联查询**

      - **确定ON或者USING子句中是否有索引**
      - 确保**GROUP BY和ORDER BY只有一个表中的列**，这样MySQL才有可能使用索引

    * 优化**子查询**

      * 用**关联查询**替代
      * 优化GROUP BY和DISTINCT
      * 这两种查询据可以使用索引来优化，是最有效的优化方法
      * 关联查询中，使用标识列分组的效率更高
      * 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序
      * WITH ROLLUP超级聚合，可以挪到应用程序处理

    * **区分in和exists， not in和not exists**

      ```sql
      select * from 表A where id in (select id from 表B);
      ```

      上面sql语句相当于

      ```sql
      select * from 表A where exists(select * from 表B where 表B.id=表A.id)
      ```

      区分in和exists主要是造成了驱动顺序的改变（这是性能变化的关键），**如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询**。所以**IN适合于外表大而内表小的情况；EXISTS适合于外表小而内表大的情况**

    * **关于not in和not exists，推荐使用not exists，不仅仅是效率问题，not in可能存在逻辑问题**

      ```sql
      select colname … from A表 where a.id not in (select b.id from B表);
      ```

      高效的sql语句

      ```sql
      select colname … from A表 Left join B表 on a.id = b.id where b.id is null;
      ```

      取出的结果为A表不在B表中的数据

  * mysql 如何查看sql语句执行时间和效率

    * 1 show profiles
    * 2 show variables;查看profiling 是否是on状态
    * 3 如果是off，则 set profiling = 1
    * 4 执行自己的sql语句
    * 5 show profiles；就可以查到sql语句的执行时间

  * **使用explain优化sql和索引?**

    * 通过explain命令可以得到**表的读取顺序**、**数据读取操作的操作类型**、**哪些索引可以使用**、**哪些索引被实际使用**、表之间的引用以及**被扫描的行数**等问题
    * **对于复杂、效率低的sql语句，我们通常是使用explain sql 来分析sql语句，这个语句可以打印出，语句的执行。这样方便我们分析，进行优化**
    * 如果发现**查询需要扫描大量的数据，但只返回少数的行**
      * 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果
      * 改变数据库和表的结构，修改数据表范式
      * 重写SQL语句，让优化器可以以更优的方式执行查询
    * table：**显示这一行的数据是关于哪张表的**
    * **type**：**这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、index和ALL**
      * all：**full table scan ;MySQL将遍历全表以找到匹配的行；**
      * **index**：index scan; index 和 all的区别在于index类型只遍历索引；
      * **range：**索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询；
      * **ref：**非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找
      * **eq_ref：**唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描；
      * **const，system：**当MySQL**对某查询某部分进行优化**，**并转为一个常量时，使用这些访问类型**。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量
    * **possible_keys：**显示**可能应用在这张表中的索引**。如果为空，没有可能的索引。可以**为相关的域从WHERE语句中选择一个合适的语句**
    * **key：** **实际使用的索引**。如果为NULL，则没有使用索引。很少的情况下，MySQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MySQL忽略索引
    * **key_len：**使用的索引的长度。在不损失精确性的情况下，长度越短越好
    * **ref：**显示索引的哪一列被使用了，如果可能的话，是一个常数
    * **rows：**MySQL认为必须检查的用来返回请求数据的行数
    * **Extra：**关于MySQL如何解析查询的额外信息

  * 索引的优化

    * 注意会**引起索引失效的情况**，以及**在适合的地方建立索引**

  * 数据库**表结构**的优化

    * 设计表时遵循**三大范式**
    * 选择合适的**数据类型**：尽可能不要存储NULL字段；使用简单的数据类型（int, varchar/ text）
    * 表的**水平切分**（Sharding）：将同一个表中的记录拆分到多个结构相同的表中（策略：哈希取模；根据ID范围来分）。当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓解单个数据库的压力
    * 表的**垂直切分**：将一张表按列切分成多个表。可以将不常用的字段单独放在同一个表中；把大字段独立放入一个表中；或者把经常使用的字段（关系密切的）放在一张表中。垂直切分之后业务更加清晰，系统之间整合或扩展容易，数据维护简单

  * 系统配置的优化操作系统

    * 增加TCP支持的队列数
    * MySQL配置文件优化：**缓存池大小和个数设置**

  * 硬件的优化磁盘性能

    * 固态硬盘
    * CPU：多核且高频
    * 内存：增大内存

* 切分

  * 水平切分

    * 水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中

    * 当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/sharding.png" alt="sharding" style="zoom:80%;" />

  * 垂直切分

    * 垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中

    * 在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的**电商数据库垂直切分成商品数据库、用户数据库**等

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/verticla_sharding.png" alt="verticla_sharding" style="zoom:90%;" />

  * Sharding 策略

    * 哈希取模：hash(key) % N；
    * 范围：可以是 ID 范围也可以是时间范围
    * 映射表：使用单独的一个数据库来存储映射关系

  * Sharding 存在的问题

    * 事务问题
      * 使用分布式事务来解决，比如 XA 接口
    * 连接
      * 可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接
    * ID 唯一性
      * 使用全局唯一 ID（GUID）
      * 为每个分片指定一个 ID 范围
      * 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)

### SQL注入

* SQL注入

  * SQL注入攻击是通过操作输入来修改SQL语句，用以达到执行代码对WEB服务器进行攻击的方法。简单的说就是在post/getweb表单、输入域名或页面请求的查询字符串中插入SQL命令，最终使web服务器执行恶意命令的过程。可以通过一个例子简单说明SQL注入攻击。假设某网站页面显示时URL为http://www.example.com?test=123，**此时URL实际向服务器传递了值为123的变量test**，这**表明当前页面是对数据库进行动态查询的结果**。由此，我们可以在URL中插入恶意的SQL语句并进行执行。另外，在网站开发过程中，**开发人员使用动态字符串构造SQL语句，用来创建所需的应用，这种情况下SQL语句在程序的执行过程中被动态的构造使用**，可以根据不同的条件产生不同的SQL语句，比如需要根据不同的要求来查询数据库中的字段。这样的开发过程其实为SQL注入攻击留下了很多的可乘之机

  * 如在没有过滤特殊字符时，出现的SQL情况

    ```sql
    $name = "Qadir'; DELETE FROM users;";
    mysqli_query($conn, "SELECT * FROM users WHERE name='{$name}'");
    ```

    以上的注入语句中，我们没有对 \$name 的变量进行过滤，$name 中插入了我们不需要的SQL语句，将删除 users 表中的所有数据。

    * **Like语句中的注入**：like查询时，如果用户输入的值有"\_"和"%"，则会出现这种情况：用户本来只是想查询"abcd"，查询结果中却有"abcd_"、"abcde"、"abcdf"等等；用户要查询"30%"（注：百分之三十）时也会出现问题。

  * 注入过程

    * 第一步：SQL注入点探测。**探测SQL注入点是关键的一步**，通过适当的分析应用程序，**可以判断什么地方存在SQL注入点**。通常**只要带有输入提交的动态网页，并且动态网页访问数据库，就可能存在SQL注入漏洞**。如果程序员信息安全意识不强，采用动态构造SQL语句访问数据库，**并且对用户的输入未进行有效性验证**，则存在SQL注入漏洞的可能性很大。一般通过页面的报错信息来确定是否存在SQL注入漏洞
    * 第二步：收集后台数据库信息。不同数据库的注入方法、函数都不尽相同，**因此在注入之前，我们先要判断一下数据库的类型**。判断数据库类型的方法很多，可以输入特殊字符，如单引号，让程序返回错误信息，我们根据错误信息提示进行判断；还可以使用特定函数来判断，比如输入“1 and version（）>0”，程序返回正常，说明version（）函数被数据库识别并执行，而version（）函数是MySQL特有的函数，因此可以推断后台数据库为MySQL
    * 第三步：**猜解用户名和密码**。**数据库中的表和字段命名一般都是有规律的**。通过构造特殊SQL语句在数据库中依次**猜解出表名、字段名、字段数、用户名和密码**
    * 第四步：查找Web后台管理入口。WEB后台管理通常不对普通用户开放，要找到后台管理的登录网址，可以利用Web目录扫描工具（如wwwscan、AWVS）快速搜索到可能的登录地址，然后逐一尝试，便可以找到后台管理平台的登录网址
    * 第五步：入侵和破坏。一般后台管理具有较高权限和较多的功能，**使用前面已破译的用户名、密码成功登录后台管理平台后，就可以任意进行破坏，比如上传木马、篡改网页、修改和窃取信息等，还可以进一步提权，入侵Web服务器和数据库服务器**

  * 检测技术

    * SQL注入的检测方式目前主要有两大类，第一：**动态监测**，即**在系统运行时，通常在系统验收阶段或上线运行阶段使用该方法，使用动态监测攻击对其系统进行扫描，然后依据扫描结果判断是否存在SQL注入漏洞**。第二：**静态检测**，又称静态代码扫描，对代码做深层次分析
      * **动态检测**：动态监测分为两类：**手工监测以及工具监测**。相对于**手动监测的高成本以及高漏检率**，在实际生产过程中更偏向于工具监测，但**工具监测同样存在较大的局限性**。其原因在于**工具是用报文来判断SQL注入是否生效**，然而**仅仅通过报文是很难精准地判断SQL注入是否存在**，因此存在较高的误报率
      * **静态检测**：静态检测的误报率相对较低，其主要原因在于**SQL注入漏洞的代码特征较为明显**
        * **使用数据库交互代码**；
        * 使用**字符串拼接方式构造动态SQL语句**；
        * **使用未过滤的不可信任数据**
      * 在常规的排查应用系统中是否存在SQL注入漏洞时，由于静态扫描的代码特征明显，误报率低和直接阅读相关代码，工作总量减少的优势，通常使用静态扫描

  * 防止SQL注入

    * 永远不要信任用户的输入。对用户的**输入进行校验**，可以**通过正则表达式**，或**限制长度**；对**单引号和 双"-"进行转换**等
    * 永远**不要使用动态拼装SQL**，可以使用**参数化的SQL或者直接使用存储过程进行数据查询存取**
    * **永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接**
    * **不要把机密信息直接存放，加密或者hash掉密码和敏感的信息**
    * **应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装**
    * SQL注入的检测方法一般采取辅助软件或网站平台来检测，软件一般采用sql注入检测工具jsky，网站平台就有亿思网站安全平台检测工具。MDCSOFT SCAN等。采用MDCSOFT-IPS可以有效的防御SQL注入，XSS攻击等
    * 分级管理
      * 对用户进行分级管理，严格控制用户的权限，**对于普通用户，禁止给予数据库建立、删除、修改等相关权限，只有系统管理员才具有增、删、改、查的权限**。例如用户在查询语句中加入了drop table。肯定是不能让其执行的，否则系统的数据库安全性就无法保障。故而通过权限的设计限制。使得即使恶意攻击者在数据提交时嵌入了相关攻击代码。但因为设置了权限，从而使得代码不能执行。从而减少SQL注入对数据库的安全威胁
    * 参数传值
      * **程序员在书写SQL语言时，禁止将变量直接写入到SQL语句**，必须**通过设置相应的参数来传递相关的变量**。从而抑制SQL注入。数据输入不能直接嵌入到查询语句中。同时要过滤输入的内容，过滤掉不安全的输入数据。或者采用参数传值的方式传递输入变量。这样可以最大程度防范SQL注入攻击
    * 基础过滤与二次过滤
      * SQL注入攻击前，入侵者通过修改参数提交“and”等特殊字符，判断是否存在漏洞，然后通过select、update等各种字符编写SQL注入语句。因此防范SQL注入**要对用户输入进行检查，确保数据输入的安全性，在具体检查输入或提交的变量时，对于单引号、双引号、冒号等字符进行转换或者过滤，从而有效防止SQL注入**。当然危险字符有很多，在获取用户输入提交的参数时，**首先要进行基础过滤，然后根据程序的功能及用户输入的可能性进行二次过滤，以确保系统的安全性**
    * 使用安全参数
      * SQL数据库为了有效抑制SQL注入攻击的影响。在进行SQL Server数据库设计时**设置了专门的SQL安全参数**。**在程序编写时应尽量使用安全参数来杜绝注入式攻击**。从而确保系统的安全性
      * SQL Server数据库提供了[Parameters集合](https://baike.baidu.com/item/Parameters集合/8058495)，它在数据库中的功能是**对数据进行类型检查和长度验证**，当程序员在程序设计时**加入了Parameters集合，系统会自动过滤掉用户输入中的执行代码，识别其为字符值**。如果**用户输入中含有恶意的代码，数据库在进行检查时也能够将其过滤掉**。同时Parameters集合还能进行**强制执行检查**。一旦检查值超出范围。系统就会出现异常报错，同时将信息发送系统管理员，方便管理员做出相应的防范措施
    * 漏洞扫描
      * 为了更有效地防范SQL注入攻击，作为系统管理除了设置有效的防范措施，**更应该及时发现系统存在SQL攻击安全漏洞**。系统管理员可以通过采购一些**专门系统的SQL漏洞扫描工具**，通过专业的扫描工具，可以及时的扫描到系统存在的相应漏洞。虽然漏洞扫描工具只能扫描到SQL注入漏洞，不能防范SQL注入攻击。但系统管理员可以通过扫描到的安全漏洞，根据不同的情况采取相应的防范措施封堵相应的漏洞，从而把SQL注入攻击的门给关上，从而确保系统的安全
    * 多层验证
      * 现在的网站系统功能越来越庞大复杂。为确保系统的安全，**访问者的数据输入必须经过严格的验证才能进入系统**，验证没通过的输入直接被拒绝访问数据库，并且向上层系统发出错误提示信息。同时在客户端访问程序中验证访问者的相关输入信息，从而更有效的防止简单的SQL注入。但是如果多层验证中的下层如果验证数据通过，那么绕过客户端的攻击者就能够随意访问系统。因此在进行多层验证时，要每个层次相互配合，只有在客户端和系统端都进行有效的验证防护，才能更好地防范SQL注入攻击
    * 数据库信息加密
      * 传统的加解密的方法大致可以分为三种
        * 对称加密：即加密方和解密方都使用相同的加密算法和密钥，这种方案的密钥的保存非常关键，**因为算法是公开的，而密钥是保密的**，一旦密匙泄露，黑客仍然可以轻易解密。常见的[对称加密算法](https://baike.baidu.com/item/对称加密算法/211953)有：AES、DES等
        * 非对称加密：即使用不同的密钥来进行加解密，密钥被分为公钥和私钥，**用私钥加密的数据必须使用公钥来解密，同样用公钥加密的数据必须用对应的私钥来解密**，常见的[非对称加密算法](https://baike.baidu.com/item/非对称加密算法/1208652)有：RSA等
        * 不可逆加密：利用哈希算法使数据加密之后无法解密回原数据，这样的哈希算法常用的有：md5、SHA-1等

## Redis

### 概述

* redis概念

  * Redis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。Redis 可以**存储键和五种不同类型的值之间的映射**。**键的类型只能为字符串，值支持五种数据类型**：**字符串、列表、集合、散列表、有序集合**
  * 与传统数据库不同的是 Redis 的数据是**存在内存中**的，所以**读写速度非常快**，因此 redis 被广泛应**用于缓存方向**，每秒可以处理超过 10万次读写操作，是**已知性能最快**的Key-Value DB。另外，Redis 也经常用来**做分布式锁**。除此之外，Redis 支持**事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案**

* redis优点

  * **读写性能优异**， redis能读的速度是110000次/s，写的速度是81000次/s
  * 支持**数据持久化**，支持AOF和RDB两种持久化方式
  * **支持事务**，Redis的所有操作都是原子性的，同时Redis还支持对**几个操作合并后的原子性执行**
  * **数据结构丰富**，除了支持string类型的value外还支持hash、set、zset、list等数据结构
  * **支持主从复制**，主机会自动将数据同步到从机，可以进行读写分离

* redis缺点

  * 数据库**容量受到物理内存的限制**，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上
  * Redis **不具备自动容错和恢复功能**，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复
  * 主机宕机，**宕机前有部分数据未能及时同步到从机**，切换IP后还会引入**数据不一致**的问题，降低了系统的可用性
  * Redis **较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂**。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费

* 跳表比红黑树优越在哪？

  * 插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度也和跳表一样，跳表不需要进行旋转等操作来维护平衡性

  * 但是，**按照区间查找数据这个操作，红黑树的效率没有跳表高，跳表时间复杂度为**$\textbf{O}(\log n)$

  * 定位区间的起点，然后在原始链表中顺序先后查找就可以实现区间查找，非常高效

  * 支持无锁操作

  * 此外，相比于红黑树，跳表还具有代码更容易实现、可读性好、不容易出错、更加灵活等优点。因此，Redis用跳表来实现

    ![skip_table](https://gitee.com/canqchen/cloudimg/raw/master/img/skip_table.png)

    在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程

    ![skip_table1](https://gitee.com/canqchen/cloudimg/raw/master/img/skip_table1.png)

* 为什么要用 Redis /为什么要用缓存

  * 主要从“高性能”和“高并发”这两点来看待这个问题

  * **高性能**：

    * *假如用户第一次访问数据库中的某些数据，这个过程会比较慢，因为是从硬盘上读取的*。将该**用户访问的数据存在数据缓存中**，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。**操作缓存就是直接操作内存，所以速度相当快**。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/redis_cache.png" alt="redis_cache" style="zoom:80%;" />

  * **高并发：**

    * **直接操作缓存能够承受的请求是远远大于直接访问数据库的**，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库

      <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/redis_cache2.png" alt="redis_cache2" style="zoom:80%;" />

* 为什么要用 Redis 而不用 map/guava 做缓存?

  * 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是**本地缓存**，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，**每个实例都需要各自保存一份缓存，缓存不具有一致性**
  * 使用 redis 或 memcached 之类的称为**分布式缓存**，在多实例的情况下，**各实例共用一份缓存数据，缓存具有一致性**。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂

* **Redis为什么这么快**

  * **完全基于内存，绝大部分请求是纯粹的内存操作，非常快速**。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是$\textbf{O}(1)$
  * **数据结构简单，对数据操作也简单**，Redis 中的数据结构是专门进行设计的
  * **采用单线程**，**避免了不必要的上下文切换和竞争条件**，**也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗**
  * **使用多路 I/O 复用模型，非阻塞 IO**
  * **使用底层模型不同**，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己**构建了 VM 机制** ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求

### 数据类型

* Redis主要有5种数据类型，包括**String，List，Set，Zset，Hash**，满足大部分的使用要求

  ![redis_data_category](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_data_category.jpg)

* STRING

  ![redis_string](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_string.png)

  ```
  > set hello world
  OK
  > get hello
  "world"
  > del hello
  (integer) 1
  > get hello
  (nil)
  ```

* LIST

  ![redis_list](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_list.png)

  ```
  > rpush list-key item
  (integer) 1
  > rpush list-key item2
  (integer) 2
  > rpush list-key item
  (integer) 3
  
  > lrange list-key 0 -1
  1) "item"
  2) "item2"
  3) "item"
  
  > lindex list-key 1
  "item2"
  
  > lpop list-key
  "item"
  
  > lrange list-key 0 -1
  1) "item2"
  2) "item"
  ```

* SET

  ![redis_set](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_set.png)

  ```
  > sadd set-key item
  (integer) 1
  > sadd set-key item2
  (integer) 1
  > sadd set-key item3
  (integer) 1
  > sadd set-key item
  (integer) 0
  
  > smembers set-key
  1) "item"
  2) "item2"
  3) "item3"
  
  > sismember set-key item4
  (integer) 0
  > sismember set-key item
  (integer) 1
  
  > srem set-key item2
  (integer) 1
  > srem set-key item2
  (integer) 0
  
  > smembers set-key
  1) "item"
  2) "item3"
  ```

* HASH

  ![redis_has](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_has.png)

  ```
  > hset hash-key sub-key1 value1
  (integer) 1
  > hset hash-key sub-key2 value2
  (integer) 1
  > hset hash-key sub-key1 value1
  (integer) 0
  
  > hgetall hash-key
  1) "sub-key1"
  2) "value1"
  3) "sub-key2"
  4) "value2"
  
  > hdel hash-key sub-key2
  (integer) 1
  > hdel hash-key sub-key2
  (integer) 0
  
  > hget hash-key sub-key1
  "value1"
  
  > hgetall hash-key
  1) "sub-key1"
  2) "value1"
  ```

* ZSET

  ![redis_zset](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_zset.png)

  ```
  > zadd zset-key 728 member1
  (integer) 1
  > zadd zset-key 982 member0
  (integer) 1
  > zadd zset-key 982 member0
  (integer) 0
  
  > zrange zset-key 0 -1 withscores
  1) "member1"
  2) "728"
  3) "member0"
  4) "982"
  
  > zrangebyscore zset-key 0 800 withscores
  1) "member1"
  2) "728"
  
  > zrem zset-key member1
  (integer) 1
  > zrem zset-key member1
  (integer) 0
  
  > zrange zset-key 0 -1 withscores
  1) "member0"
  2) "982"
  ```

### redis应用场景

* 应用总结一
  * **计数器**
    * 可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。
  * **缓存**
    * **将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率**
  * **会话缓存**
    * 可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性
  * **全页缓存（FPC）**
    * 除基本的会话token之外，Redis还提供很简便的FPC平台。以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。
  * **查找表**
    * 例如 DNS 记录就很适合使用 Redis 进行存储。查找表和缓存类似，也是利用了 **Redis 快速的查找特性**。**但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源**
  * **消息队列(发布/订阅功能)**
    * List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息。不过最好使用 Kafka、RabbitMQ 等消息中间件
  * **分布式锁实现**
    * **在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步**。可以使用 Redis 自带的 **SETNX 命令实现分布式锁**，除此之外，还可以使用官方提供的 **RedLock 分布式锁**实现
  * **其它**
    * **Set 可以实现交集、并集等操作，从而实现共同好友等功能**。**ZSet 可以实现有序性操作，从而实现排行榜等功能**
* 应用总结二
  * Redis相比其他缓存，有一个非常大的优势，就是**支持多种数据类型**
  * 数据类型说明string字符串，最简单的k-v存储hash-hash格式，value为field和value，适合ID-Detail这样的场景。list简单的list，顺序列表，支持首位或者末尾插入数据set无序list，查找速度快，适合交集、并集、差集处理sorted set有序的set
  * 其实，通过上面的数据类型的特性，基本就能想到合适的应用场景了
  * string——适合最简单的k-v存储，类似于memcached的存储结构，短信验证码，配置信息等，就用这种类型来存储
  * hash——一般key为ID或者唯一标示，value对应的就是详情了。如商品详情，个人信息详情，新闻详情等。
  * list——因为list是有序的，比较适合存储一些有序且数据相对固定的数据。如省市区表、字典表等。因为list是有序的，适合根据写入的时间来排序，如：最新的***，消息队列等
  * set——可以简单的理解为ID-List的模式，如微博中一个人有哪些好友，set最牛的地方在于，可以对两个set提供交集、并集、差集操作。例如：查找两个人共同的好友等
  * Sorted Set——是set的增强版本，**增加了一个score参数，自动会根据score的值进行排序**。比较适合类似于**top 10**等**不根据插入的时间来排序的数据**
  * 如上所述，虽然Redis不像关系数据库那么复杂的数据结构，但是，也能适合很多场景，比一般的缓存数据结构要多。了解每种数据结构适合的业务场景，不仅有利于提升开发效率，也能有效利用Redis的性能

### redis持久化

* 持久化概念
  
  * 持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失
* Redis 的持久化机制是什么？各自的优缺点？
  * Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制
  
  * RDB：是Redis DataBase缩写快照。**RDB是Redis默认的持久化方式**。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期
    * 优点
      * 只有一个文件 dump.rdb，方便持久化
      * 容灾性好，一个文件可以保存到安全的磁盘
      * 性能最大化，**fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化**。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能
      * 相对于数据集大时，比 AOF 的启动效率更高
    * 缺点
      * **数据安全性低**。RDB 是**间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失**。所以这种方式更适合数据要求不严谨的时候)
    
  * AOF（Append-only file)持久化方式： 是指所有的命令行记录以 redis 命令请求协议的格式完全持久化存储，保存为 aof 文件。**将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据**
  
  * 使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：
  
    ![redis_aof](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_aof.png)
  
    * always 选项会严重减低服务器的性能；
    * everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响；
    * no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量
    * 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令
  
  * **当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复**
  
  * 优点
    * 数据安全，aof 持久化可以配置 append fsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次
    * 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题
    * AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令进行合并重写），可以删除其中的某些冗余写命令（比如误操作的 flush all）
    
  * 缺点
    * AOF 文件比 RDB 文件大，且恢复速度慢
    * 数据集大的时候，比 RDB 启动效率低
    
  * 二者比较
    * AOF文件比RDB更新频率高，优先使用AOF还原数据
    * AOF比RDB更安全也更大
    * RDB性能比AOF好
    * 如果两个都配了优先加载AOF
* 如何选择合适的持久化方式
  * 一般来说， 如果想达到足以媲美PostgreSQL的**数据安全性**，你**应该同时使用两种持久化功能**。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，**因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整**
  * 如果你**非常关心你的数据**， 但**仍然可以承受数分钟以内的数据丢失**，那么你可以只使用RDB持久化
  * 有很多用户都**只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug**
  * **如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式**
* **Redis持久化数据和缓存怎么做扩容？**
  * 如果Redis被当做缓存使用，使用**一致性哈希实现动态扩容缩容**
  * 如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样

### redis过期键删除策略

* Redis是key-value数据库，我们**可以设置Redis中缓存的key的过期时间**。**Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理**。
  * 过期策略通常有以下三种：
    * 定时过期：**每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除**。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量
    * 惰性过期：**只有当访问一个key时，才会判断该key是否已过期，过期则清除**。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存
    * 定期过期：**每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key**。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键)
  * Redis中同时使用了惰性过期和定期过期两种过期策略
* Redis key的过期时间和永久有效分别怎么设置？
  * EXPIRE和PERSIST命令
* 通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢?
  * 除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求**进行自定义的缓存淘汰**，常见的策略有两种：
    * 定时去清理过期的缓存
    * 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。
  * 两者各有优劣，第一种的缺点是**维护大量缓存的key是比较麻烦的**，第二种的缺点就是**每次用户请求过来都要判断缓存失效**，逻辑相对比较复杂！具体用哪种方案，可以根据自己的应用场景来权衡

### redis内存相关

* MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据
  * redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略
* redis内存淘汰策略
  * Redis的内存淘汰策略是指在**Redis的用于缓存的内存不足时**，**怎么处理需要新写入且需要申请额外空间的数据**
  * **全局的键空间选择性移除**
    * noeviction：当内存不足以容纳新写入数据时，**新写入操作会报错**
    * allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，**移除最近最少使用的key**（**这个是最常用的**）
    * allkeys-random：当内存不足以容纳新写入数据时，在键空间中，**随机移除某个key**
  * **设置过期时间的键空间选择性移除**
    * volatile-lru：当内存不足以容纳新写入数据时，**在设置了过期时间的键空间中，移除最近最少使用的key**
    * volatile-random：当内存不足以容纳新写入数据时，**在设置了过期时间的键空间中，随机移除某个key**
    * volatile-ttl：当内存不足以容纳新写入数据时，**在设置了过期时间的键空间中，有更早过期时间的key优先移除**
  * **Redis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰**
  * 总结
    * Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据
* Redis主要消耗什么物理资源？Redis的内存用完了会发生什么？
  * **内存**。如果达到设置的上限，Redis的**写命令会返回错误信息**（但是读命令还可以正常返回。）或者可以**配置内存淘汰机制**，当Redis达到内存上限时会冲刷掉旧的内容
* Redis如何做内存优化？
  * 可以好好利用Hash,list,sorted set,set等集合类型数据，因为**通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以应该尽可能的将你的数据模型抽象到一个散列表里面**。比如web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是**应该把这个用户的所有信息存储到一张散列表里面**

### redis线程模型

* Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型
  * 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。
  * 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件
* **虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性**

### redis事务

* **Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合**。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。总结说：**redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令**

* redis事务三阶段

  * 事务开始 MULTI
  * 命令入队
  * 事务执行 EXEC

  事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队

* Redis事务相关命令

  * Redis事务功能是通过**MULTI、EXEC、DISCARD和WATCH 四个原语**实现的，**Redis会将一个事务中的所有命令序列化，然后按顺序执行**
  * **redis 不支持回滚**，“Redis 在**事务失败时不进行回滚，而是继续执行余下的命令**”， 所以 Redis 的内部可以保持简单且快速
  * **如果在一个事务中的命令出现错误，那么所有的命令都不会执行**
  * **如果在一个事务中出现运行错误，那么正确的命令会被执行**
  * **WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为**。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令
  * **MULTI命令用于开启一个事务，它总是返回OK**。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行
  * EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 
  * 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出
  * UNWATCH命令可以取消WATCH对所有key的监控

* **Redis的事务总是具有ACID中的一致性和隔离性**，**其他特性是不支持的**。当服务器运行在*AOF*持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性

* Redis 是单进程程序，并且它**保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止**。因此，**Redis 的事务是总是带有隔离性的**

* Redis中，单条命令是原子性执行的，但**事务不保证原子性，且没有回滚**。事务中**任意命令执行失败，其余的命令仍会被执行**

### redis集群方案

* 哨兵模式

  <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/sentinel.jpg" alt="sentinel" style="zoom:67%;" />

  * 哨兵概念：sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能：
    * **集群监控**：负责监控 redis master 和 slave 进程是否正常工作
    * **消息通知**：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员
    * **故障转移**：如果 master node 挂掉了，会自动转移到 slave node 上
    * **配置中心**：如果故障转移发生了，通知 client 客户端新的 master 地址
  * **哨兵用于实现 redis 集群的高可用**，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作
  * 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题
  * 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就不可靠
  * **哨兵的核心知识**
    - 哨兵至少需要 3 个实例，来保证自己的健壮性
    - 哨兵 + redis 主从的部署架构，是**不保证数据零丢失**的，只能保证 redis 集群的高可用性
    - 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练

* 官方Redis Cluster 方案(服务端路由查询)

  <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/redis_cluster.png" alt="redis_cluster" style="zoom:80%;" />

  * Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行
  * 方案说明

    * 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位
    * 每份数据分片会存储在多个互为主从的多节点上
    * 数据写入先写主节点，再同步到从节点(支持配置为阻塞同步)
    * 同一分片多个节点间的数据不保持一致性
    * 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点
    * 扩容时时需要需要把旧节点的数据迁移一部分到新节点
  * 在 redis cluster 架构下，**每个 redis 要放开两个端口号**，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379
  * **16379 端口号是用来进行节点间通信的**，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，**gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间**
  * **节点间的内部通信机制**
    * 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信
  * **分布式寻址算法**
    - hash 算法（大量缓存重建）
    - 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）
    - redis cluster 的 hash slot 算法
  * 优点
    * 无中心架构，支持动态扩容，对业务透明
    * 具备Sentinel的监控和自动Failover(故障转移)能力
    * 客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可
    * 高性能，客户端直连redis服务，免去了proxy代理的损耗
  * 缺点
    * 运维也很复杂，数据迁移需要人工干预
    * 只能使用0号数据库
    * 不支持批量操作(pipeline管道操作)
    * 分布式逻辑和存储模块耦合等
  
* **基于代理服务器分片**

  <img src="https://gitee.com/canqchen/cloudimg/raw/master/img/redis_proxy.png" alt="redis_proxy" style="zoom:60%;" />

  * 简介：**客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端**
  * 特征
    * 透明接入，业务程序不用关心后端Redis实例，切换成本低
    * Proxy 的逻辑和存储的逻辑是隔离的
    * 代理层多了一次转发，性能有所损耗

* **redis主从架构**

  * 单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，**一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读**。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发

  * redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

    ![redis_master_slave](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_master_slave.png)

  * **redis replication 的核心机制**

    * redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；
    * 一个 master node 是可以配置**多个** slave node 的
    * slave node 也可以连接其他的 slave node
    * slave node 做复制的时候，不会 block master node 的正常工作
    * slave node 在做复制的时候，也不会 block 对自己的查询操作，它会**用旧的数据集来提供服务**；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了
    * slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量

  * 注意，如果采用了主从架构，那么建议**必须开启 master node 的持久化**，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你**关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了**

  * 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空

  * **redis 主从复制的核心原理**

    * 当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node

    * 如果这是 slave node **初次连接**到 master node，那么会触发一次` full resynchronization `全量复制。此时 master 会**启动一个后台线程**，开始生成一份 **RDB 快照文件**，同时还会将从客户端 client 新收到的所有写命令缓存在内存中

    * RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会**先写入本地磁盘**，然后**再从本地磁盘加载到内存中**，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据

      ![redis_master_slave2](https://gitee.com/canqchen/cloudimg/raw/master/img/redis_master_slave2.png)

  * **过程原理**

    * 当从库和主库建立MS关系后，会向主数据库发送SYNC命令
    * 主库接收到SYNC命令后会开始在后台保存快照(RDB持久化过程)，并将期间接收到的写命令缓存起来
    * 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis
    * 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令
    * 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致

  * 缺点

    * 所有的slave节点数据的复制和同步都由master节点来处理，会**造成master节点压力太大**，使用主从从结构来解决

* 生产环境中的 redis 是怎么部署的？

  * redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s
  * 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题
  * 5 台机器对外提供读写，一共有 50g 内存
  * 因为**每个主实例都挂了一个从实例**，所以是高可用的，任何一个主实例宕机，都会**自动故障迁移**，**redis 从实例会自动变成主实例继续提供读写服务**
  * 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量
  * 其实大型的公司，会有基础架构的 team 负责缓存集群的运维

* 说说Redis哈希槽的概念？

  * Redis集群**没有使用一致性hash**，而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽

* Redis集群会有写操作丢失吗？为什么？

  * Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作

* Redis集群之间是如何复制的？

  * **异步复制**

* Redis集群最大节点个数是多少？

  * 16384个

* Redis集群如何选择数据库？

  * Redis集群目前无法做数据库选择，默认在0数据库

### redis分区

* Redis是单线程的，如何提高多核CPU的利用率？
  * 可以在同一个服务器**部署多个Redis的实例**，并把他们**当作不同的服务器来使用**，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个CPU，你可以考虑一下分片（shard）
* 为什么要做Redis分区？
  * 分区可以让Redis**管理更大的内存**，Redis将**可以使用所有机器的内存**。如果没有分区，你最多只能使用一台机器的内存。分区使Redis的计算能力通过简单地增加计算机得到成倍提升，Redis的网络带宽也会随着计算机和网卡的增加而成倍增长
* Redis分区实现方案
  * **客户端分区**：就是**在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取**。大多数客户端已经实现了客户端分区
  * **代理分区**：意味着客户端将请求发送给代理，然后**代理决定去哪个节点写数据或者读数据**。**代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端**。redis和memcached的一种代理实现就是Twemproxy
  * 查询路由(Query routing)：客户端**随机地请求任意一个redis实例**，然后由Redis将请求**转发给正确的Redis节点**。Redis Cluster实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接redirected到正确的redis节点

* Redis分区有什么缺点？
  * 涉及多个key的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）
  * 同时操作多个key，则不能使用Redis事务.
  * 分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集
  * 当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件
  * 分区时动态扩容或缩容可能非常复杂。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题

### redis分布式问题

* Redis实现分布式锁
  * Redis为**单进程单线程模式，采用队列模式将并发访问变成串行访问**，且多客户端对Redis的连接并不存在竞争关系，Redis中可以使用**SETNX**命令实现分布式锁
  * 当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作
  * SETNX 是『**SET if Not eXists**』(**如果不存在，则 SET**)的简写
  * 返回值：设置成功，返回 1 。设置失败，返回 0
  * 使用SETNX完成同步锁的流程及事项如下：

    * 使用SETNX命令获取锁，若返回0（key已存在，锁已存在）则获取失败，反之获取成功
    * 为了**防止获取锁后程序出现异常**，**导致其他线程/进程调用SETNX命令总是返回0而进入死锁状态，需要为该key设置一个“合理”的过期时间**
    * 释放锁，使用DEL命令将锁数据删除
* 什么是 RedLock
  * Redis 官方站提出了一种**权威的基于 Redis 实现分布式锁的方式名叫 Redlock**，此种方式比原先的单节点的方法更安全。它可以保证以下特性：
    * **安全特性**：互斥访问，即永远只有一个 client 能拿到锁
    * **避免死锁**：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区
    * **容错性**：只要大部分 Redis 节点存活就可以正常提供服务
* 如何解决 Redis 的并发竞争 Key 问题
  * 所谓 Redis 的并发竞争 Key 的问题也就是**多个系统同时对一个 key 进行操作**，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同
  * 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）
  * 基于**zookeeper**临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁
  * 在实践中，当然是从以可靠性为主。所以首推Zookeeper
* 分布式Redis是前期做还是后期规模上来了再做好？为什么？
  * 既然Redis是如此的轻量（单实例只使用1M内存），**为防止以后的扩容，最好的办法就是一开始就启动较多实例**。即便你只有一台服务器，你也可以一开始就让Redis以分布式的方式运行，使用分区，在同一台服务器上启动多个实例
  * 一开始就多设置几个Redis实例，例如32或者64个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的
  * 这样的话，当你的数据不断增长，需要更多的Redis服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的Redis实例从第一台机器迁移到第二台机器

### redis缓存异常

* 缓存雪崩

  * 缓存雪崩是指**缓存同一时间大面积的失效**，所以，**后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉**

  * 解决方案
    * 缓存数据的**过期时间设置随机**，防止同一时间大量数据过期现象发生
    * 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队
    * 给每一个缓存数据**增加相应的缓存标记**，记录缓存的是否失效，**如果缓存标记失效，则更新数据缓存**

* 缓存穿透

  * 缓存穿透是指**缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉**

  * 解决方案

    * 接口层**增加校验**，如用户鉴权校验，id做基础校验，id<=0的直接拦截
    * 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击
    * 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力

  * 附加

    * 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。
    * Bitmap： 典型的就是哈希表
    * 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了

    * 布隆过滤器（推荐）
      * 就是引入了k(k>1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程
      * 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难
      * Bloom-Filter算法的核心思想就是**利用多个不同的Hash函数来解决“冲突”**
      * Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，**如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中**。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想
      * Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在

* 缓存击穿

  * 缓存击穿是指**缓存中没有但数据库中有的数据**（一般是缓存时间到期），这时由于**并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力**。和缓存雪崩不同的是，缓存击穿指**并发查同一条数据**，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库

  * 解决方案
    * 设置热点数据永远不过期
    * 加互斥锁

* 缓存预热

  * 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！

  * 解决方案
    * 直接写个缓存刷新页面，上线时手工操作一下
    * 数据量不大，可以在项目启动的时候自动进行加载
    * 定时刷新缓存

* 缓存降级

  * 当**访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务**。系统可以**根据一些关键数据进行自动降级，也可以配置开关实现人工降级**

  * 缓存降级的最终目的是**保证核心服务可用，即使是有损的**。而且有些服务是无法降级的（如加入购物车、结算）

  * 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：
    * 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级
    * 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警
    * 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级
    * 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级

  * 服务降级的目的，是为了**防止Redis服务故障，导致数据库跟着一起发生雪崩问题**。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，**Redis出现问题，不去数据库查询，而是直接返回默认值给用户**

* 热点数据和冷数据

  * 热点数据，缓存才有价值

  * 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。**频繁修改的数据，看情况考虑使用缓存**

  * 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，**缓存以后可能读取数十万次**。再举个例子，某导航产品，我们将导航信息，缓存以后可能**读取数百万次**

  * **数据更新前至少读取两次，缓存才有意义**。这个是最基本的策略，**如果缓存还没有起作用就失效了，那就没有太大价值了**

  * **那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢**？有！比如，**这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力**

* 缓存热点key

  * 缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮

  * 解决方案
    * 对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询

### redis其他问题

* Redis与Memcached的区别

  * 两者都是非关系型内存键值数据库，现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！Redis 与 Memcached 主要有以下不同

    ![diff](https://gitee.com/canqchen/cloudimg/raw/master/img/diff.jpg)

  * memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型

  * redis的速度比memcached快很多

  * redis可以持久化其数据

* 如何保证缓存与数据库双写时的数据一致性？

  * 只要用到缓存，就可能会涉及到**缓存与数据库双存储双写**，你只要是双写，就一定会有数据一致性的问题
  * 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，**读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况**
  * 串行化之后，就会导致系统的**吞吐量会大幅度的降低**，用比正常情况下多几倍的机器去支撑线上的一个请求
  * 还有一种方式就是**可能会暂时产生不一致的情况**，但是发生的几率特别小，就是**先更新数据库，然后再删除缓存**

  ![rd_wr_accord](https://gitee.com/canqchen/cloudimg/raw/master/img/rd_wr_accord.png)

* Redis常见性能问题和解决方案？
  * Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化
  * 如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次
  * 为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内
  * 尽量避免在压力较大的主库上增加从库
  * Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象
  * 为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master<–Slave1<–Slave2<–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变
* 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？
  * 使用keys指令可以扫出指定模式的key列表
  * 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？
  * 这个时候你要回答redis关键的一个特性：**redis的单线程的**。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复
  * 这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长
* 使用Redis做过异步队列吗，是如何实现的
  
  * 使用list类型保存数据信息，**rpush生产消息**，**lpop消费消息**，当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息，如果不想sleep的话，可以使用blpop, 在没有信息的时候，会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失
* Redis如何实现延时队列
  
  * 使用sortedset，使用时间戳做score，消息内容作为key，调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理